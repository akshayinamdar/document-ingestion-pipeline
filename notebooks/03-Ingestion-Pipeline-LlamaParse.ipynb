{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Install required packages (run this first if you get import errors)\n",
    "# Uncomment and run this cell if you need to install the required packages\n",
    "\n",
    "# !pip install llama-cloud-services\n",
    "# !pip install langchain langchain-community langchain-huggingface\n",
    "# !pip install faiss-cpu sentence-transformers\n",
    "# !pip install nest-asyncio\n",
    "\n",
    "print(\"üì¶ Required packages for LlamaParse pipeline:\")\n",
    "print(\"  - llama-cloud-services (LlamaParse API)\")\n",
    "print(\"  - langchain langchain-community (document processing)\")  \n",
    "print(\"  - langchain-huggingface (embeddings)\")\n",
    "print(\"  - faiss-cpu (vector store)\")\n",
    "print(\"  - sentence-transformers (embedding models)\")\n",
    "print(\"  - nest-asyncio (fixes Jupyter async issues)\")\n",
    "print(\"‚úÖ If you see import errors, uncomment the pip install commands above and run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197b0c7",
   "metadata": {},
   "source": [
    "# LlamaParse Document Ingestion Pipeline\n",
    "\n",
    "This notebook implements a document ingestion pipeline using **LlamaParse**, a GenAI-native document parser that excels at parsing complex documents with tables, visual elements, and varied layouts.\n",
    "\n",
    "## Key Features of LlamaParse:\n",
    "- ‚úÖ **Broad file type support**: .pdf, .pptx, .docx, .xlsx, .html\n",
    "- ‚úÖ **Table recognition**: Accurate parsing of embedded tables\n",
    "- ‚úÖ **Multimodal parsing**: Extraction of visual elements\n",
    "- ‚úÖ **Custom parsing**: Configurable output formatting\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Setup**: Import libraries and configure LlamaParse API\n",
    "2. **Discovery**: Scan for PDF documents in the docs folder\n",
    "3. **Parsing**: Process documents using LlamaParse API\n",
    "4. **Conversion**: Transform results to LangChain Document format\n",
    "5. **Chunking**: Split documents into manageable chunks\n",
    "6. **Vectorization**: Create embeddings and vector store for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7327866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace-new\\Deutsche B√∂rse\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LlamaParse Document Ingestion Pipeline\n",
      "============================================================\n",
      "‚úì Libraries imported successfully\n",
      "‚úì API key configured\n",
      "‚úì Nested async support enabled for Jupyter\n",
      "‚úì EU region configured\n",
      "‚úì Environment ready for document processing\n",
      "‚úì Timestamp: 2025-09-25 17:01:12\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries and configure LlamaParse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Fix for nested async in Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# LlamaParse imports\n",
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "# LangChain imports for document processing and vector store\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Configure LlamaParse API key\n",
    "LLAMA_CLOUD_API_KEY = \"llx-MHXNHhQO6ahnPlDx7i5O3QYgdYdaVmhtWIFyrJPI1zszoSu8\"\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_CLOUD_API_KEY\n",
    "\n",
    "# Configure for EU region (based on previous successful test)\n",
    "os.environ[\"LLAMA_CLOUD_BASE_URL\"] = \"https://api.cloud.eu.llamaindex.ai\"\n",
    "\n",
    "print(\"üöÄ LlamaParse Document Ingestion Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úì Libraries imported successfully\")\n",
    "print(f\"‚úì API key configured\")\n",
    "print(f\"‚úì Nested async support enabled for Jupyter\")\n",
    "print(f\"‚úì EU region configured\")\n",
    "print(f\"‚úì Environment ready for document processing\")\n",
    "print(f\"‚úì Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e62514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LlamaParse parser...\n",
      "==================================================\n",
      "‚úì LlamaParse parser initialized\n",
      "  - Result type: markdown\n",
      "\n",
      "Scanning 'docs' folder for PDF documents...\n",
      "‚úì Found 6 PDF files:\n",
      "  1. aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf (247.7 KB)\n",
      "  2. Canadian Collateral Management Services (CCMS).pdf (95.5 KB)\n",
      "  3. cbl-aml-questionnaire-data.pdf (1495.4 KB)\n",
      "  4. Disclosure Requirements ‚Äì Investment Funds ‚ÄìDenmark.pdf (90.4 KB)\n",
      "  5. Holding Restrictions ‚Äì Investment Funds ‚Äì Ireland.pdf (83.1 KB)\n",
      "  6. Holding Restrictions ‚Äì Investment Funds ‚ÄìDenmark.pdf (83.1 KB)\n",
      "\n",
      "üìÅ Ready to process 6 documents with LlamaParse\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize LlamaParse and discover PDF documents\n",
    "print(\"Initializing LlamaParse parser...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize LlamaParse with optimized settings\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    result_type=\"markdown\",  # Get structured markdown output\n",
    "    num_workers=3,  # Process multiple files in parallel\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    "    # Updated: Use system_prompt instead of deprecated parsing_instruction\n",
    "    system_prompt=\"Focus on extracting structured data, tables, and key financial information. Preserve document hierarchy and formatting.\"\n",
    ")\n",
    "\n",
    "print(\"‚úì LlamaParse parser initialized\")\n",
    "print(f\"  - Result type: markdown\")\n",
    "\n",
    "# Discover PDF files in docs folder\n",
    "docs_folder = \"docs\"\n",
    "pdf_files = []\n",
    "\n",
    "print(f\"\\nScanning '{docs_folder}' folder for PDF documents...\")\n",
    "if os.path.exists(docs_folder):\n",
    "    for file in os.listdir(docs_folder):\n",
    "        if file.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(docs_folder, file)\n",
    "            pdf_files.append(pdf_path)\n",
    "    \n",
    "    print(f\"‚úì Found {len(pdf_files)} PDF files:\")\n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        file_size = os.path.getsize(pdf_file) / 1024  # Size in KB\n",
    "        print(f\"  {i}. {os.path.basename(pdf_file)} ({file_size:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Warning: '{docs_folder}' folder not found!\")\n",
    "    print(\"Please ensure your PDF documents are in the 'docs' folder.\")\n",
    "\n",
    "print(f\"\\nüìÅ Ready to process {len(pdf_files)} documents with LlamaParse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c245a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2b: Test API Connection with a small document\n",
    "print(\"üß™ Testing LlamaParse API connection...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if pdf_files:\n",
    "    # Test with the smallest PDF file first\n",
    "    test_file = min(pdf_files, key=lambda f: os.path.getsize(f))\n",
    "    test_file_size = os.path.getsize(test_file) / 1024\n",
    "    \n",
    "    print(f\"üìÑ Testing with smallest file: {os.path.basename(test_file)} ({test_file_size:.1f} KB)\")\n",
    "    print(f\"üîÑ Sending test request to LlamaParse API...\")\n",
    "    \n",
    "    try:\n",
    "        # Test parsing with just 1 page to verify API works\n",
    "        test_result = parser.parse(test_file)\n",
    "        \n",
    "        if test_result and test_result.pages:\n",
    "            print(f\"‚úÖ API test successful!\")\n",
    "            print(f\"üìä Test result: {len(test_result.pages)} pages processed\")\n",
    "            print(f\"üìù Sample content length: {len(test_result.pages[0].md) if test_result.pages[0].md else 0} characters\")\n",
    "            \n",
    "            # Show a small preview\n",
    "            if test_result.pages[0].md:\n",
    "                preview = test_result.pages[0].md[:150].strip()\n",
    "                print(f\"üìã Content preview: {preview}...\")\n",
    "            \n",
    "            print(f\"\\nüéâ API connection verified! Ready to process all documents.\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  API connected but no content extracted from test file\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå API test failed: {error_msg}\")\n",
    "        \n",
    "        if \"Invalid API Key\" in error_msg:\n",
    "            print(f\"\\nüîß API Key Issue Troubleshooting:\")\n",
    "            print(f\"1. Check your API key at: https://cloud.llamaindex.ai/api-key\")\n",
    "            print(f\"2. Verify you have remaining credits in your account\")\n",
    "            print(f\"3. Try the EU region if needed: os.environ['LLAMA_CLOUD_BASE_URL'] = 'https://api.cloud.eu.llamaindex.ai'\")\n",
    "        elif \"quota\" in error_msg.lower() or \"limit\" in error_msg.lower():\n",
    "            print(f\"\\nüìä Quota Issue:\")\n",
    "            print(f\"You may have exceeded your daily/monthly limits\")\n",
    "            print(f\"Check your usage at: https://cloud.llamaindex.ai/\")\n",
    "        else:\n",
    "            print(f\"\\nüõ†Ô∏è  Other possible solutions:\")\n",
    "            print(f\"1. Check your internet connection\")\n",
    "            print(f\"2. Verify the file is not corrupted\")\n",
    "            print(f\"3. Try again in a few minutes (temporary service issues)\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No PDF files found to test with\")\n",
    "    print(\"Please add PDF files to the 'docs' folder first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating fresh parser instance with async fix applied...\n",
      "‚úÖ Fresh parser created with single worker and async support\n",
      "\n",
      "Starting LlamaParse document processing...\n",
      "============================================================\n",
      "\n",
      "üìÑ Processing file 1/6: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "----------------------------------------\n",
      "  üîÑ Sending to LlamaParse API...\n",
      "Started parsing the file under job_id f41bd005-c254-4767-b713-9640e1ce8b0a\n",
      "  ‚úÖ Successfully parsed!\n",
      "  üìä Pages processed: 7\n",
      "  üìù Sample content: # Clearstream Banking S.A. - Customer Due Diligence Statement\n",
      "\n",
      "## 1. Entity Information\n",
      "\n",
      "| **Full Legal Name**               | **Legal Form**        | **Regulatory Status**...\n",
      "\n",
      "üìÑ Processing file 2/6: Canadian Collateral Management Services (CCMS).pdf\n",
      "----------------------------------------\n",
      "  üîÑ Sending to LlamaParse API...\n",
      "  ‚ùå Error parsing docs\\Canadian Collateral Management Services (CCMS).pdf: Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.\n",
      "\n",
      "üìÑ Processing file 3/6: cbl-aml-questionnaire-data.pdf\n",
      "----------------------------------------\n",
      "  üîÑ Sending to LlamaParse API...\n",
      "Started parsing the file under job_id db0592e9-c7ad-4cdc-ad92-0c718bea0bf8\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Process documents using LlamaParse API\n",
    "# Apply nest_asyncio fix right before processing\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create a fresh parser instance to ensure async fix is applied\n",
    "print(\"üîß Creating fresh parser instance with async fix applied...\")\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    result_type=\"markdown\",\n",
    "    num_workers=2,  # Keep single worker to avoid concurrency issues\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    "    system_prompt=\"Focus on extracting structured data, tables, and key financial information. Preserve document hierarchy and formatting.\"\n",
    ")\n",
    "print(\"‚úÖ Fresh parser created with single worker and async support\")\n",
    "\n",
    "print(\"\\nStarting LlamaParse document processing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Storage for parsed results and processing statistics\n",
    "parsed_results = []\n",
    "failed_files = []\n",
    "processing_stats = {\n",
    "    'total_files': len(pdf_files),\n",
    "    'successful': 0,\n",
    "    'failed': 0,\n",
    "    'total_pages': 0,\n",
    "    'processing_time': 0\n",
    "}\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "if pdf_files:\n",
    "    # Process files with LlamaParse\n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"\\nüìÑ Processing file {i}/{len(pdf_files)}: {os.path.basename(pdf_file)}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Parse the PDF file\n",
    "            print(f\"  üîÑ Sending to LlamaParse API...\")\n",
    "            result = parser.parse(pdf_file)\n",
    "            \n",
    "            # Extract information from the result\n",
    "            if result and result.pages:\n",
    "                print(f\"  ‚úÖ Successfully parsed!\")\n",
    "                print(f\"  üìä Pages processed: {len(result.pages)}\")\n",
    "                \n",
    "                # Store result with metadata\n",
    "                parsed_results.append({\n",
    "                    'file_path': pdf_file,\n",
    "                    'file_name': os.path.basename(pdf_file),\n",
    "                    'result': result,\n",
    "                    'pages': len(result.pages),\n",
    "                    'processed_at': datetime.now()\n",
    "                })\n",
    "                \n",
    "                processing_stats['successful'] += 1\n",
    "                processing_stats['total_pages'] += len(result.pages)\n",
    "                \n",
    "                # Show sample content from first page\n",
    "                if result.pages[0].md:\n",
    "                    sample_content = result.pages[0].md[:200].strip()\n",
    "                    print(f\"  üìù Sample content: {sample_content}...\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Warning: No content extracted from {pdf_file}\")\n",
    "                failed_files.append((pdf_file, \"No content extracted\"))\n",
    "                processing_stats['failed'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"  ‚ùå Error parsing {pdf_file}: {error_msg}\")\n",
    "            failed_files.append((pdf_file, error_msg))\n",
    "            processing_stats['failed'] += 1\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_stats['processing_time'] = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Display processing summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä LLAMAPARSE PROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Successfully processed: {processing_stats['successful']} files\")\n",
    "print(f\"‚ùå Failed to process: {processing_stats['failed']} files\")\n",
    "print(f\"üìÑ Total pages extracted: {processing_stats['total_pages']}\")\n",
    "print(f\"‚è±Ô∏è  Total processing time: {processing_stats['processing_time']:.2f} seconds\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\n‚ùå Failed files:\")\n",
    "    for file, error in failed_files:\n",
    "        print(f\"  - {os.path.basename(file)}: {error}\")\n",
    "\n",
    "if processing_stats['successful'] > 0:\n",
    "    avg_time = processing_stats['processing_time'] / processing_stats['successful']\n",
    "    print(f\"üìà Average time per file: {avg_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nüéâ LlamaParse processing complete! Ready for document conversion...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117608a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3a: Retry failed files with fresh parser (after nest_asyncio fix)\n",
    "print(\"üîÑ Retrying failed files with fresh parser instance...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reinitialize parser to ensure nest_asyncio fix is applied\n",
    "parser_fresh = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    result_type=\"markdown\",\n",
    "    num_workers=1,  # Use 1 worker to avoid concurrency issues\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    "    system_prompt=\"Focus on extracting structured data, tables, and key financial information. Preserve document hierarchy and formatting.\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fresh parser initialized with single worker\")\n",
    "\n",
    "# Get list of failed files\n",
    "retry_files = [file_path for file_path, error in failed_files if \"nested async\" in error]\n",
    "print(f\"üìÑ Retrying {len(retry_files)} files that failed due to async issues\")\n",
    "\n",
    "retry_start_time = datetime.now()\n",
    "\n",
    "for i, pdf_file in enumerate(retry_files, 1):\n",
    "    print(f\"\\nüìÑ Retry {i}/{len(retry_files)}: {os.path.basename(pdf_file)}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        print(f\"  üîÑ Sending to LlamaParse API (single worker)...\")\n",
    "        result = parser_fresh.parse(pdf_file)\n",
    "        \n",
    "        if result and result.pages:\n",
    "            print(f\"  ‚úÖ Successfully parsed on retry!\")\n",
    "            print(f\"  üìä Pages processed: {len(result.pages)}\")\n",
    "            \n",
    "            # Store result with metadata\n",
    "            parsed_results.append({\n",
    "                'file_path': pdf_file,\n",
    "                'file_name': os.path.basename(pdf_file),\n",
    "                'result': result,\n",
    "                'pages': len(result.pages),\n",
    "                'processed_at': datetime.now()\n",
    "            })\n",
    "            \n",
    "            # Update processing stats\n",
    "            processing_stats['successful'] += 1\n",
    "            processing_stats['failed'] -= 1\n",
    "            processing_stats['total_pages'] += len(result.pages)\n",
    "            \n",
    "            # Remove from failed files list\n",
    "            failed_files[:] = [(f, e) for f, e in failed_files if f != pdf_file]\n",
    "            \n",
    "            # Show sample content\n",
    "            if result.pages[0].md:\n",
    "                sample_content = result.pages[0].md[:200].strip()\n",
    "                print(f\"  üìù Sample content: {sample_content}...\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  No content extracted on retry\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"  ‚ùå Still failed on retry: {error_msg}\")\n",
    "\n",
    "retry_end_time = datetime.now()\n",
    "retry_duration = (retry_end_time - retry_start_time).total_seconds()\n",
    "\n",
    "# Update total processing time\n",
    "processing_stats['processing_time'] += retry_duration\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ RETRY PROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÑ Files retried: {len(retry_files)}\")\n",
    "print(f\"‚è±Ô∏è  Retry processing time: {retry_duration:.2f} seconds\")\n",
    "print(f\"‚úÖ Final successful files: {processing_stats['successful']}\")\n",
    "print(f\"‚ùå Final failed files: {processing_stats['failed']}\")\n",
    "print(f\"üìÑ Total pages extracted: {processing_stats['total_pages']}\")\n",
    "\n",
    "if processing_stats['failed'] == 0:\n",
    "    print(f\"\\nüéâ All files processed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Still have {processing_stats['failed']} failed files:\")\n",
    "    for file, error in failed_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to proceed with {processing_stats['successful']} successfully parsed documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Convert LlamaParse results to LangChain Document objects\n",
    "print(\"Converting LlamaParse results to LangChain Document format...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_documents = []\n",
    "document_stats = {\n",
    "    'total_documents': 0,\n",
    "    'total_content_length': 0,\n",
    "    'files_processed': 0\n",
    "}\n",
    "\n",
    "for parsed_result in parsed_results:\n",
    "    file_name = parsed_result['file_name']\n",
    "    file_path = parsed_result['file_path']\n",
    "    result = parsed_result['result']\n",
    "    \n",
    "    print(f\"\\nüìÑ Converting {file_name}...\")\n",
    "    \n",
    "    # Process each page from LlamaParse result\n",
    "    for page_num, page in enumerate(result.pages, 1):\n",
    "        # Use markdown content if available, fallback to text\n",
    "        content = page.md if page.md else page.text\n",
    "        \n",
    "        if content and content.strip():\n",
    "            # Create rich metadata for each document\n",
    "            metadata = {\n",
    "                'source_file': file_path,\n",
    "                'file_name': file_name,\n",
    "                'page': page_num,\n",
    "                'total_pages': len(result.pages),\n",
    "                'parser': 'LlamaParse',\n",
    "                'result_type': 'markdown',\n",
    "                'processed_at': parsed_result['processed_at'].isoformat(),\n",
    "                'content_length': len(content)\n",
    "            }\n",
    "            \n",
    "            # Add layout information if available\n",
    "            if hasattr(page, 'layout') and page.layout:\n",
    "                metadata['has_layout'] = True\n",
    "                \n",
    "            # Add structured data information if available\n",
    "            if hasattr(page, 'structuredData') and page.structuredData:\n",
    "                metadata['has_structured_data'] = True\n",
    "                \n",
    "            # Add image information if available\n",
    "            if hasattr(page, 'images') and page.images:\n",
    "                metadata['image_count'] = len(page.images)\n",
    "                metadata['has_images'] = True\n",
    "            \n",
    "            # Create LangChain Document\n",
    "            document = Document(\n",
    "                page_content=content,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            \n",
    "            all_documents.append(document)\n",
    "            document_stats['total_content_length'] += len(content)\n",
    "    \n",
    "    document_stats['files_processed'] += 1\n",
    "    print(f\"  ‚úÖ Converted {len(result.pages)} pages to LangChain Documents\")\n",
    "\n",
    "document_stats['total_documents'] = len(all_documents)\n",
    "\n",
    "# Display conversion summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DOCUMENT CONVERSION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Files processed: {document_stats['files_processed']}\")\n",
    "print(f\"üìÑ Total documents created: {document_stats['total_documents']}\")\n",
    "print(f\"üìä Total content length: {document_stats['total_content_length']:,} characters\")\n",
    "\n",
    "if document_stats['total_documents'] > 0:\n",
    "    avg_length = document_stats['total_content_length'] / document_stats['total_documents']\n",
    "    print(f\"üìà Average document length: {avg_length:.0f} characters\")\n",
    "\n",
    "# Show sample metadata\n",
    "if all_documents:\n",
    "    print(f\"\\nüìù Sample document metadata:\")\n",
    "    sample_doc = all_documents[0]\n",
    "    for key, value in list(sample_doc.metadata.items())[:8]:  # Show first 8 metadata fields\n",
    "        print(f\"  {key}: {value}\")\n",
    "    if len(sample_doc.metadata) > 8:\n",
    "        print(f\"  ... and {len(sample_doc.metadata) - 8} more fields\")\n",
    "\n",
    "print(f\"\\nüéâ Document conversion complete! Ready for chunking and vectorization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e659719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Split documents into chunks for optimal RAG performance\n",
    "print(\"Splitting documents into chunks for RAG optimization...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize text splitter with optimized settings for LlamaParse content\n",
    "# Using larger chunks since LlamaParse provides well-structured content\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,        # Larger chunks for structured markdown content\n",
    "    chunk_overlap=300,      # Higher overlap to preserve context across chunks\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n## \",          # Split on markdown headers first\n",
    "        \"\\n### \",         # Then subheaders\n",
    "        \"\\n\\n\",          # Paragraph breaks\n",
    "        \"\\n\",            # Line breaks\n",
    "        \".\",             # Sentences\n",
    "        \" \",             # Words\n",
    "        \"\"               # Characters\n",
    "    ],\n",
    "    keep_separator=True     # Keep separators to maintain markdown structure\n",
    ")\n",
    "\n",
    "print(\"Text Splitter Configuration:\")\n",
    "print(f\"  üìè Chunk size: {text_splitter._chunk_size} characters\")\n",
    "print(f\"  üîó Chunk overlap: {text_splitter._chunk_overlap} characters\")\n",
    "print(f\"  üîß Separators optimized for markdown content\")\n",
    "print(f\"  üìä Hierarchical splitting: Headers ‚Üí Paragraphs ‚Üí Sentences\")\n",
    "\n",
    "# Split all documents\n",
    "print(f\"\\nüîÑ Splitting {len(all_documents)} documents into chunks...\")\n",
    "chunked_documents = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Analyze chunking results\n",
    "chunk_sizes = [len(doc.page_content) for doc in chunked_documents]\n",
    "chunk_stats = {\n",
    "    'original_documents': len(all_documents),\n",
    "    'chunked_documents': len(chunked_documents),\n",
    "    'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,\n",
    "    'min_chunk_size': min(chunk_sizes) if chunk_sizes else 0,\n",
    "    'max_chunk_size': max(chunk_sizes) if chunk_sizes else 0,\n",
    "    'total_content': sum(chunk_sizes)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä DOCUMENT CHUNKING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÑ Original documents: {chunk_stats['original_documents']}\")\n",
    "print(f\"üß© Generated chunks: {chunk_stats['chunked_documents']}\")\n",
    "print(f\"üìè Average chunk size: {chunk_stats['avg_chunk_size']:.0f} characters\")\n",
    "print(f\"üìâ Minimum chunk size: {chunk_stats['min_chunk_size']} characters\")\n",
    "print(f\"üìà Maximum chunk size: {chunk_stats['max_chunk_size']} characters\")\n",
    "print(f\"üìä Total content: {chunk_stats['total_content']:,} characters\")\n",
    "\n",
    "if chunk_stats['original_documents'] > 0:\n",
    "    expansion_ratio = chunk_stats['chunked_documents'] / chunk_stats['original_documents']\n",
    "    print(f\"üìà Chunk expansion ratio: {expansion_ratio:.1f}x\")\n",
    "\n",
    "# Show distribution of chunk sizes\n",
    "size_ranges = {\n",
    "    'Small (0-500)': len([s for s in chunk_sizes if s <= 500]),\n",
    "    'Medium (501-1000)': len([s for s in chunk_sizes if 501 <= s <= 1000]),\n",
    "    'Large (1001-1500)': len([s for s in chunk_sizes if 1001 <= s <= 1500]),\n",
    "    'Extra Large (1500+)': len([s for s in chunk_sizes if s > 1500])\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Chunk size distribution:\")\n",
    "for range_name, count in size_ranges.items():\n",
    "    percentage = (count / len(chunk_sizes) * 100) if chunk_sizes else 0\n",
    "    print(f\"  {range_name}: {count} chunks ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Document chunking complete! Ready for embedding generation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3857b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create embeddings and vector store using HuggingFace\n",
    "print(\"Creating embeddings and building vector store...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize HuggingFace embeddings with a high-quality model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"ü§ñ Initializing embedding model: {model_name}\")\n",
    "print(\"   Model characteristics:\")\n",
    "print(\"   - Optimized for semantic search and clustering\")\n",
    "print(\"   - 384-dimensional embeddings\")\n",
    "print(\"   - Fast inference on CPU\")\n",
    "print(\"   - Excellent performance on financial/business documents\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Embedding model loaded successfully\")\n",
    "print(f\"üìä Embedding dimension: 384\")\n",
    "print(f\"üíª Device: CPU\")\n",
    "print(f\"üéØ Normalization: Enabled\")\n",
    "\n",
    "# Create vector store from chunked documents\n",
    "print(f\"\\nüîÑ Generating embeddings for {len(chunked_documents)} document chunks...\")\n",
    "print(\"‚è±Ô∏è  This process may take a few minutes depending on document count...\")\n",
    "\n",
    "embedding_start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Create FAISS vector store\n",
    "    vectorstore = FAISS.from_documents(chunked_documents, embeddings)\n",
    "    \n",
    "    embedding_end_time = datetime.now()\n",
    "    embedding_duration = (embedding_end_time - embedding_start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\nüéâ Vector store created successfully!\")\n",
    "    print(f\"‚è±Ô∏è  Embedding generation time: {embedding_duration:.2f} seconds\")\n",
    "    print(f\"üìä Average time per chunk: {embedding_duration/len(chunked_documents):.3f} seconds\")\n",
    "    print(f\"üóÇÔ∏è  Vector store contains {len(chunked_documents)} embedded chunks\")\n",
    "    print(f\"üéØ Ready for semantic search and retrieval!\")\n",
    "    \n",
    "    # Calculate vector store statistics\n",
    "    print(f\"\\nüìà Vector Store Statistics:\")\n",
    "    print(f\"  - Total vectors: {len(chunked_documents)}\")\n",
    "    print(f\"  - Embedding dimension: 384\")\n",
    "    print(f\"  - Index type: FAISS (Facebook AI Similarity Search)\")\n",
    "    print(f\"  - Memory usage: ~{len(chunked_documents) * 384 * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating embeddings: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Ensure sentence-transformers is installed: pip install sentence-transformers\")\n",
    "    print(\"2. Check available memory (large documents may require more RAM)\")\n",
    "    print(\"3. Consider reducing chunk_size if memory issues persist\")\n",
    "    print(\"4. For GPU acceleration: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    \n",
    "print(f\"\\nüöÄ Vector store ready for testing and querying!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test document retrieval with semantic search\n",
    "print(\"Testing document retrieval system...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define test queries relevant to Deutsche B√∂rse and financial documents\n",
    "test_queries = [\n",
    "    \"investment fund regulations and compliance requirements\",\n",
    "    \"trading restrictions and market access rules\", \n",
    "    \"risk management framework and procedures\",\n",
    "    \"financial reporting standards and disclosure\",\n",
    "    \"market data and trading infrastructure\",\n",
    "    \"regulatory compliance and supervision\"\n",
    "]\n",
    "\n",
    "print(f\"üîç Testing semantic search with {len(test_queries)} queries:\")\n",
    "print(f\"üìÑ Searching across {len(chunked_documents)} document chunks\")\n",
    "print(f\"üéØ Retrieving top 3 most relevant results per query\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*20} Query {i}/{len(test_queries)} {'='*20}\")\n",
    "    print(f\"üîç Query: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Perform similarity search\n",
    "        relevant_docs = vectorstore.similarity_search(query, k=3)\n",
    "        \n",
    "        if relevant_docs:\n",
    "            for j, doc in enumerate(relevant_docs, 1):\n",
    "                # Extract metadata\n",
    "                source_file = doc.metadata.get('file_name', 'Unknown')\n",
    "                page_num = doc.metadata.get('page', 'Unknown')\n",
    "                parser_type = doc.metadata.get('parser', 'Unknown')\n",
    "                content_length = doc.metadata.get('content_length', len(doc.page_content))\n",
    "                \n",
    "                print(f\"  üìã Result {j}:\")\n",
    "                print(f\"    üìÑ Source: {source_file}\")\n",
    "                print(f\"    üìñ Page: {page_num}\")\n",
    "                print(f\"    ü§ñ Parser: {parser_type}\")\n",
    "                print(f\"    üìè Length: {content_length} chars\")\n",
    "                \n",
    "                # Show content preview (first 200 characters)\n",
    "                preview = doc.page_content[:200].strip()\n",
    "                # Clean up markdown formatting for preview\n",
    "                preview = preview.replace('#', '').replace('**', '').replace('*', '')\n",
    "                print(f\"    üìù Preview: {preview}...\")\n",
    "                print(f\"    {'-'*50}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  No relevant documents found for this query\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error during search: {e}\")\n",
    "\n",
    "# Test similarity search with scores\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ SIMILARITY SCORE ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "sample_query = test_queries[0]  # Use first query for detailed analysis\n",
    "print(f\"Sample query: '{sample_query}'\")\n",
    "\n",
    "try:\n",
    "    # Get documents with similarity scores\n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(sample_query, k=5)\n",
    "    \n",
    "    print(f\"\\nüìä Top 5 results with similarity scores:\")\n",
    "    for i, (doc, score) in enumerate(docs_with_scores, 1):\n",
    "        source = doc.metadata.get('file_name', 'Unknown')\n",
    "        page = doc.metadata.get('page', 'Unknown')\n",
    "        print(f\"  {i}. Score: {score:.4f} | {source} (Page {page})\")\n",
    "    \n",
    "    # Analyze score distribution\n",
    "    scores = [score for _, score in docs_with_scores]\n",
    "    if scores:\n",
    "        print(f\"\\nüìà Score statistics:\")\n",
    "        print(f\"  Best match: {min(scores):.4f}\")\n",
    "        print(f\"  Worst match: {max(scores):.4f}\")\n",
    "        print(f\"  Score range: {max(scores) - min(scores):.4f}\")\n",
    "        print(f\"  Average score: {sum(scores) / len(scores):.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in similarity score analysis: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Document retrieval testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save vector store and processed data for future use\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"Saving vector store and processed data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a timestamp for this processing session\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "session_id = f\"llamaparse_{timestamp}\"\n",
    "\n",
    "try:\n",
    "    # Save FAISS vector store to disk\n",
    "    vector_store_path = \"vector_store_llamaparse\"\n",
    "    vectorstore.save_local(vector_store_path)\n",
    "    print(f\"‚úÖ Vector store saved to '{vector_store_path}' folder\")\n",
    "    \n",
    "    # Save processed documents with enhanced metadata\n",
    "    documents_file = f\"processed_documents_llamaparse_{timestamp}.pkl\"\n",
    "    with open(documents_file, \"wb\") as f:\n",
    "        pickle.dump(chunked_documents, f)\n",
    "    print(f\"‚úÖ Processed documents saved to '{documents_file}'\")\n",
    "    \n",
    "    # Save detailed processing configuration and statistics\n",
    "    processing_config = {\n",
    "        'session_id': session_id,\n",
    "        'timestamp': timestamp,\n",
    "        'parser_config': {\n",
    "            'parser_type': 'LlamaParse',\n",
    "            'api_key_prefix': LLAMA_CLOUD_API_KEY[:10] + \"...\",  # Don't store full API key\n",
    "            'result_type': 'markdown',\n",
    "            'num_workers': 4,\n",
    "            'language': 'en'\n",
    "        },\n",
    "        'chunking_config': {\n",
    "            'chunk_size': 1500,\n",
    "            'chunk_overlap': 300,\n",
    "            'separators': ['\\\\n## ', '\\\\n### ', '\\\\n\\\\n', '\\\\n', '.', ' ', ''],\n",
    "            'keep_separator': True\n",
    "        },\n",
    "        'embedding_config': {\n",
    "            'model_name': model_name,\n",
    "            'embedding_dimension': 384,\n",
    "            'device': 'cpu',\n",
    "            'normalize_embeddings': True\n",
    "        },\n",
    "        'processing_stats': processing_stats,\n",
    "        'document_stats': document_stats,\n",
    "        'chunk_stats': chunk_stats,\n",
    "        'files_processed': [result['file_name'] for result in parsed_results],\n",
    "        'total_processing_time': processing_stats['processing_time']\n",
    "    }\n",
    "    \n",
    "    config_file = f\"llamaparse_config_{timestamp}.json\"\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(processing_config, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Processing configuration saved to '{config_file}'\")\n",
    "    \n",
    "    # Save raw LlamaParse results for future reference\n",
    "    llamaparse_results_file = f\"llamaparse_results_{timestamp}.pkl\"\n",
    "    with open(llamaparse_results_file, \"wb\") as f:\n",
    "        pickle.dump(parsed_results, f)\n",
    "    print(f\"‚úÖ Raw LlamaParse results saved to '{llamaparse_results_file}'\")\n",
    "    \n",
    "    # Create a summary report\n",
    "    report_file = f\"processing_report_{timestamp}.md\"\n",
    "    with open(report_file, \"w\") as f:\n",
    "        f.write(f\"# LlamaParse Processing Report\\\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\")\n",
    "        f.write(f\"## Summary\\\\n\")\n",
    "        f.write(f\"- **Session ID**: {session_id}\\\\n\")\n",
    "        f.write(f\"- **Files Processed**: {processing_stats['successful']}/{processing_stats['total_files']}\\\\n\")\n",
    "        f.write(f\"- **Total Pages**: {processing_stats['total_pages']}\\\\n\")\n",
    "        f.write(f\"- **Documents Created**: {document_stats['total_documents']}\\\\n\")\n",
    "        f.write(f\"- **Chunks Generated**: {chunk_stats['chunked_documents']}\\\\n\")\n",
    "        f.write(f\"- **Processing Time**: {processing_stats['processing_time']:.2f} seconds\\\\n\\\\n\")\n",
    "        f.write(f\"## Files Processed\\\\n\")\n",
    "        for result in parsed_results:\n",
    "            f.write(f\"- {result['file_name']} ({result['pages']} pages)\\\\n\")\n",
    "        if failed_files:\n",
    "            f.write(f\"\\\\n## Failed Files\\\\n\")\n",
    "            for file, error in failed_files:\n",
    "                f.write(f\"- {os.path.basename(file)}: {error}\\\\n\")\n",
    "        f.write(f\"\\\\n## Configuration\\\\n\")\n",
    "        f.write(f\"- **Parser**: LlamaParse (markdown output)\\\\n\")\n",
    "        f.write(f\"- **Chunking**: {chunk_stats['chunked_documents']} chunks, avg {chunk_stats['avg_chunk_size']:.0f} chars\\\\n\")\n",
    "        f.write(f\"- **Embeddings**: {model_name} (384-dim)\\\\n\")\n",
    "        f.write(f\"- **Vector Store**: FAISS\\\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Processing report saved to '{report_file}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving data: {e}\")\n",
    "    print(\"Please check file permissions and available disk space.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ LLAMAPARSE INGESTION PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Successfully processed {processing_stats['successful']} PDF files\")\n",
    "print(f\"‚úÖ Generated {processing_stats['total_pages']} pages of structured content\")\n",
    "print(f\"‚úÖ Created {document_stats['total_documents']} LangChain documents\")\n",
    "print(f\"‚úÖ Split into {chunk_stats['chunked_documents']} optimized chunks\")\n",
    "print(f\"‚úÖ Built vector store with 384-dimensional embeddings\")\n",
    "print(f\"‚úÖ Total processing time: {processing_stats['processing_time']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\\\nüìÇ Output Files:\")\n",
    "print(f\"  - Vector store: {vector_store_path}/\")\n",
    "print(f\"  - Documents: {documents_file}\")\n",
    "print(f\"  - Configuration: {config_file}\")\n",
    "print(f\"  - Report: {report_file}\")\n",
    "\n",
    "print(f\"\\\\nüöÄ **Ready for RAG Implementation!**\")\n",
    "print(f\"Your vector store is now ready to be used in RAG pipelines.\")\n",
    "print(f\"Use the saved configuration to ensure consistency across applications.\")\n",
    "\n",
    "print(f\"\\\\nüí° **Next Steps:**\")\n",
    "print(f\"1. Load the vector store: vectorstore = FAISS.load_local('{vector_store_path}', embeddings)\")\n",
    "print(f\"2. Implement your RAG query pipeline\")\n",
    "print(f\"3. Connect to your preferred LLM (GPT, Claude, Llama, etc.)\")\n",
    "print(f\"4. Build your conversational AI application\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
