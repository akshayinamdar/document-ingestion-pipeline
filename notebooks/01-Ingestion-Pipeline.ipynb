{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d59e0-fef8-4792-a2ed-a60442f50f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document.\n",
      "\n",
      "--- Document Content ---\n",
      "The Clearstream Banking AG (CBF) is a German Central Securities Depository (CSD). It provides post-trade infrastructure for the German securities market.\n",
      "Key services include settlement, custody, and asset servicing. Settlement is the process of transferring securities and funds between parties. Custody involves the safekeeping and administration of securities on behalf of clients.\n",
      "The primary system used for these services is CASCADE. All participants must adhere to the rules and regulations set forth by BaFin.\n",
      "This gives us a simple, multi-paragraph document to work with.\n",
      "\n",
      "--- Document Metadata ---\n",
      "{'source': 'sample_doc.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import the necessary loader and load the document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# The loader needs the path to our file\n",
    "loader = TextLoader(\"sample_doc.txt\")\n",
    "\n",
    "# The .load() method reads the file and creates a Document object\n",
    "documents = loader.load()\n",
    "\n",
    "# Let's inspect the result\n",
    "print(f\"Loaded {len(documents)} document.\")\n",
    "print(\"\\n--- Document Content ---\")\n",
    "print(documents[0].page_content)\n",
    "\n",
    "print(\"\\n--- Document Metadata ---\")\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8669a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace-new\\Deutsche BÃ¶rse\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import additional loaders and utilities for PDF processing\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a08691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 PDF files in the docs folder:\n",
      "  - docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  - docs\\Canadian Collateral Management Services (CCMS).pdf\n",
      "  - docs\\cbl-aml-questionnaire-data.pdf\n",
      "  - docs\\Disclosure Requirements â€“ Investment Funds â€“Denmark.pdf\n",
      "  - docs\\Holding Restrictions â€“ Investment Funds â€“ Ireland.pdf\n",
      "  - docs\\Holding Restrictions â€“ Investment Funds â€“Denmark.pdf\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define the docs folder path and scan for PDF files\n",
    "docs_folder = \"docs\"\n",
    "pdf_files = []\n",
    "\n",
    "# Check if docs folder exists\n",
    "if os.path.exists(docs_folder):\n",
    "    # Get all PDF files in the docs folder\n",
    "    for file in os.listdir(docs_folder):\n",
    "        if file.lower().endswith('.pdf'):\n",
    "            pdf_files.append(os.path.join(docs_folder, file))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files in the docs folder:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"  - {pdf_file}\")\n",
    "else:\n",
    "    print(f\"Error: '{docs_folder}' folder not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad496290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PDF ingestion process...\n",
      "==================================================\n",
      "Loading: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  âœ“ Successfully loaded 7 pages\n",
      "Loading: docs\\Canadian Collateral Management Services (CCMS).pdf\n",
      "  âœ“ Successfully loaded 3 pages\n",
      "Loading: docs\\cbl-aml-questionnaire-data.pdf\n",
      "  âœ“ Successfully loaded 13 pages\n",
      "Loading: docs\\Disclosure Requirements â€“ Investment Funds â€“Denmark.pdf\n",
      "  âœ“ Successfully loaded 3 pages\n",
      "Loading: docs\\Holding Restrictions â€“ Investment Funds â€“ Ireland.pdf\n",
      "  âœ“ Successfully loaded 2 pages\n",
      "Loading: docs\\Holding Restrictions â€“ Investment Funds â€“Denmark.pdf\n",
      "  âœ“ Successfully loaded 2 pages\n",
      "==================================================\n",
      "Ingestion complete!\n",
      "Total documents loaded: 30\n",
      "Failed files: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load all PDF documents\n",
    "all_documents = []\n",
    "failed_files = []\n",
    "\n",
    "print(\"Starting PDF ingestion process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        print(f\"Loading: {pdf_file}\")\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add source information to metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata['source_file'] = pdf_file\n",
    "            doc.metadata['file_name'] = os.path.basename(pdf_file)\n",
    "        \n",
    "        all_documents.extend(documents)\n",
    "        print(f\"  âœ“ Successfully loaded {len(documents)} pages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_files.append((pdf_file, str(e)))\n",
    "        print(f\"  âœ— Failed to load {pdf_file}: {e}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ingestion complete!\")\n",
    "print(f\"Total documents loaded: {len(all_documents)}\")\n",
    "print(f\"Failed files: {len(failed_files)}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for file, error in failed_files:\n",
    "        print(f\"  - {file}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7799e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Analysis Summary\n",
      "============================================================\n",
      "\n",
      "File: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  Pages: 7\n",
      "  Total characters: 15,640\n",
      "  Sample content: 1 \n",
      "To the attention  of: The Transfer Agent (â€œTAâ€) / the Fund  and applicable to all \n",
      "Clearstream Banking SA accounts and designations in funds under the administration \n",
      "of the TA \n",
      "Clearstream Banking...\n",
      "----------------------------------------\n",
      "\n",
      "File: Canadian Collateral Management Services (CCMS).pdf\n",
      "  Pages: 3\n",
      "  Total characters: 3,196\n",
      "  Sample content: Canadian Collateral Management Services(CCMS)\n",
      "Powering domestic collateral management with a world-leading tripartyinfrastructure\n",
      "08.07.2025\n",
      "Continuous regulatory changes require ï¬nancial market infra...\n",
      "----------------------------------------\n",
      "\n",
      "File: cbl-aml-questionnaire-data.pdf\n",
      "  Pages: 13\n",
      "  Total characters: 32,581\n",
      "  Sample content: Wolfsberg Group Correspondent Banking Due Diligence Questionnaire (CBDDQ) V1.4\n",
      "Â© The Wolfsberg Group 2023 Page 1 CBDDQ V1.4\n",
      "Financial Institution Name:\n",
      "Location (Country) :\n",
      "No # Question Answer\n",
      "1 Full...\n",
      "----------------------------------------\n",
      "\n",
      "File: Disclosure Requirements â€“ Investment Funds â€“Denmark.pdf\n",
      "  Pages: 3\n",
      "  Total characters: 4,169\n",
      "  Sample content: Disclosure Requirements â€“ Investment Funds â€“Denmark\n",
      "26.06.2025\n",
      "Disclosure Category: 2\n",
      "Clearstream Banking S.A. (â€œCBLâ€) may be required, upon request, to disclosethe identity and holdings of clients an...\n",
      "----------------------------------------\n",
      "\n",
      "File: Holding Restrictions â€“ Investment Funds â€“ Ireland.pdf\n",
      "  Pages: 2\n",
      "  Total characters: 2,537\n",
      "  Sample content: Holding Restrictions â€“ Investment Funds â€“Ireland\n",
      "10.09.2025\n",
      "Restrictions on clients\n",
      "CBL clients domiciled in Ireland are not allowed to hold the Irish investmentfunds on the Register market through CB...\n",
      "----------------------------------------\n",
      "\n",
      "File: Holding Restrictions â€“ Investment Funds â€“Denmark.pdf\n",
      "  Pages: 2\n",
      "  Total characters: 2,828\n",
      "  Sample content: Holding Restrictions â€“ Investment Funds â€“Denmark\n",
      "24.06.2025\n",
      "Restrictions on clients\n",
      "No general restrictions on client residency for holdings of investment funds heldthrough CBL. However, certain funds...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Analyze and display document summaries\n",
    "print(\"Document Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group documents by file\n",
    "files_summary = {}\n",
    "for doc in all_documents:\n",
    "    file_name = doc.metadata.get('file_name', 'Unknown')\n",
    "    if file_name not in files_summary:\n",
    "        files_summary[file_name] = {\n",
    "            'page_count': 0,\n",
    "            'total_chars': 0,\n",
    "            'sample_content': ''\n",
    "        }\n",
    "    \n",
    "    files_summary[file_name]['page_count'] += 1\n",
    "    files_summary[file_name]['total_chars'] += len(doc.page_content)\n",
    "    \n",
    "    # Store sample content from first page if not already stored\n",
    "    if not files_summary[file_name]['sample_content']:\n",
    "        # Get first 200 characters as sample\n",
    "        files_summary[file_name]['sample_content'] = doc.page_content[:200].strip()\n",
    "\n",
    "# Display summary for each file\n",
    "for file_name, summary in files_summary.items():\n",
    "    print(f\"\\nFile: {file_name}\")\n",
    "    print(f\"  Pages: {summary['page_count']}\")\n",
    "    print(f\"  Total characters: {summary['total_chars']:,}\")\n",
    "    print(f\"  Sample content: {summary['sample_content']}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec66bc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Metadata Inspection\n",
      "============================================================\n",
      "\n",
      "Document 1 Metadata:\n",
      "  producer: MicrosoftÂ® Word for Microsoft 365\n",
      "  creator: MicrosoftÂ® Word for Microsoft 365\n",
      "  creationdate: 2024-04-03T17:34:32+02:00\n",
      "  author: Grace O'Connor\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_actionid: db4963dd-f6a7-4a29-baa3-f43e6f4be2dd\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_contentbits: 2\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_enabled: true\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_method: Privileged\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_name: 2e952e98-911c-4aff-840a-f71bc6baaf7f\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_setdate: 2023-04-27T15:43:42Z\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_siteid: e00ddcdf-1e0f-4be5-a37a-894a4731986a\n",
      "  moddate: 2024-04-11T12:53:52+02:00\n",
      "  source: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  total_pages: 7\n",
      "  page: 0\n",
      "  page_label: 1\n",
      "  source_file: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  file_name: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  Content length: 1830 characters\n",
      "\n",
      "Document 2 Metadata:\n",
      "  producer: MicrosoftÂ® Word for Microsoft 365\n",
      "  creator: MicrosoftÂ® Word for Microsoft 365\n",
      "  creationdate: 2024-04-03T17:34:32+02:00\n",
      "  author: Grace O'Connor\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_actionid: db4963dd-f6a7-4a29-baa3-f43e6f4be2dd\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_contentbits: 2\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_enabled: true\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_method: Privileged\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_name: 2e952e98-911c-4aff-840a-f71bc6baaf7f\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_setdate: 2023-04-27T15:43:42Z\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_siteid: e00ddcdf-1e0f-4be5-a37a-894a4731986a\n",
      "  moddate: 2024-04-11T12:53:52+02:00\n",
      "  source: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  total_pages: 7\n",
      "  page: 1\n",
      "  page_label: 2\n",
      "  source_file: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  file_name: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  Content length: 2368 characters\n",
      "\n",
      "Document 3 Metadata:\n",
      "  producer: MicrosoftÂ® Word for Microsoft 365\n",
      "  creator: MicrosoftÂ® Word for Microsoft 365\n",
      "  creationdate: 2024-04-03T17:34:32+02:00\n",
      "  author: Grace O'Connor\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_actionid: db4963dd-f6a7-4a29-baa3-f43e6f4be2dd\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_contentbits: 2\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_enabled: true\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_method: Privileged\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_name: 2e952e98-911c-4aff-840a-f71bc6baaf7f\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_setdate: 2023-04-27T15:43:42Z\n",
      "  msip_label_2e952e98-911c-4aff-840a-f71bc6baaf7f_siteid: e00ddcdf-1e0f-4be5-a37a-894a4731986a\n",
      "  moddate: 2024-04-11T12:53:52+02:00\n",
      "  source: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  total_pages: 7\n",
      "  page: 2\n",
      "  page_label: 3\n",
      "  source_file: docs\\aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  file_name: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "  Content length: 1928 characters\n",
      "\n",
      "... and 27 more documents\n",
      "\n",
      "All 30 documents are now loaded and ready for processing!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Display detailed metadata for inspection\n",
    "print(\"Detailed Metadata Inspection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show metadata for first few documents as examples\n",
    "for i, doc in enumerate(all_documents[:3]):  # Show first 3 documents\n",
    "    print(f\"\\nDocument {i+1} Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"  Content length: {len(doc.page_content)} characters\")\n",
    "    \n",
    "if len(all_documents) > 3:\n",
    "    print(f\"\\n... and {len(all_documents) - 3} more documents\")\n",
    "\n",
    "# Store documents for later use\n",
    "print(f\"\\nAll {len(all_documents)} documents are now loaded and ready for processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af8ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks...\n",
      "==================================================\n",
      "Original documents: 30\n",
      "After chunking: 87\n",
      "Average chunk size: 802 characters\n",
      "Min chunk size: 183 characters\n",
      "Max chunk size: 999 characters\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Split documents into chunks for better RAG performance\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Splitting documents into chunks...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Size of each chunk in characters\n",
    "    chunk_overlap=200,      # Overlap between chunks to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split all documents\n",
    "chunked_documents = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Original documents: {len(all_documents)}\")\n",
    "print(f\"After chunking: {len(chunked_documents)}\")\n",
    "\n",
    "# Show chunking statistics\n",
    "chunk_sizes = [len(doc.page_content) for doc in chunked_documents]\n",
    "print(f\"Average chunk size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\n",
    "print(f\"Min chunk size: {min(chunk_sizes)} characters\")\n",
    "print(f\"Max chunk size: {max(chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8d7bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and vector store with HuggingFace...\n",
      "==================================================\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n",
      "Device: CPU\n",
      "\n",
      "Generating embeddings for all document chunks...\n",
      "This may take a few minutes depending on the number of chunks...\n",
      "âœ“ Vector store created successfully!\n",
      "âœ“ Embedded 87 document chunks\n",
      "âœ“ Ready for semantic search and retrieval!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Create embeddings and vector store using HuggingFace\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"Creating embeddings and vector store with HuggingFace...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize HuggingFace embeddings with all-MiniLM-L6-v2\n",
    "# This model is specifically designed for semantic search and clustering\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity scores\n",
    ")\n",
    "\n",
    "print(\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Embedding dimension: 384\")\n",
    "print(\"Device: CPU\")\n",
    "print()\n",
    "\n",
    "# Create vector store from documents\n",
    "print(\"Generating embeddings for all document chunks...\")\n",
    "print(\"This may take a few minutes depending on the number of chunks...\")\n",
    "\n",
    "try:\n",
    "    vectorstore = FAISS.from_documents(chunked_documents, embeddings)\n",
    "    \n",
    "    print(f\"âœ“ Vector store created successfully!\")\n",
    "    print(f\"âœ“ Embedded {len(chunked_documents)} document chunks\")\n",
    "    print(\"âœ“ Ready for semantic search and retrieval!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error creating embeddings: {e}\")\n",
    "    print(\"Make sure sentence-transformers is installed: pip install sentence-transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7845178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing document retrieval...\n",
      "==================================================\n",
      "\n",
      "Query: 'investment fund regulations'\n",
      "  Result 1:\n",
      "    Source: Disclosure Requirements â€“ Investment Funds â€“Denmark.pdf\n",
      "    Page: 1\n",
      "    Content preview: Holding Restrictions â€“ Investment Funds â€“Denmark\n",
      "Investment Fund Market Guide - Denmark\n",
      "î˜‰î˜‰î˜‰\n",
      "9/23/25, 4:23 PM Disclosure Requirements â€“ Investment Fund...\n",
      "    --------------------------------------------------\n",
      "  Result 2:\n",
      "    Source: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "    Page: 0\n",
      "    Content preview: Investment Fund Services and the Global Liquidity Hub.  \n",
      "2. Applicable   Regulations\n",
      "As a Monetary Financial Institution (MFI) established in Luxembou...\n",
      "    --------------------------------------------------\n",
      "\n",
      "Query: 'trading restrictions'\n",
      "  Result 1:\n",
      "    Source: Holding Restrictions â€“ Investment Funds â€“Denmark.pdf\n",
      "    Page: 0\n",
      "    Content preview: Holding Restrictions â€“ Investment Funds â€“Denmark\n",
      "24.06.2025\n",
      "Restrictions on clients\n",
      "No general restrictions on client residency for holdings of invest...\n",
      "    --------------------------------------------------\n",
      "  Result 2:\n",
      "    Source: Holding Restrictions â€“ Investment Funds â€“ Ireland.pdf\n",
      "    Page: 0\n",
      "    Content preview: Holding Restrictions â€“ Investment Funds â€“Ireland\n",
      "10.09.2025\n",
      "Restrictions on clients\n",
      "CBL clients domiciled in Ireland are not allowed to hold the Irish...\n",
      "    --------------------------------------------------\n",
      "\n",
      "Query: 'compliance requirements'\n",
      "  Result 1:\n",
      "    Source: cbl-aml-questionnaire-data.pdf\n",
      "    Page: 7\n",
      "    Content preview: 76 g Marijuana-related Entities\n",
      "76 h MSB/MVTS customers\n",
      "76 i Non-account customers\n",
      "76 j Non-Government Organisations\n",
      "76 k Non-resident customers\n",
      "76 l...\n",
      "    --------------------------------------------------\n",
      "  Result 2:\n",
      "    Source: cbl-aml-questionnaire-data.pdf\n",
      "    Page: 9\n",
      "    Content preview: Wolfsberg Group Correspondent Banking Due Diligence Questionnaire (CBDDQ) V1.4\n",
      "Â© The Wolfsberg Group 2023 Page 10 CBDDQ V1.4\n",
      "93 Does the Entity have p...\n",
      "    --------------------------------------------------\n",
      "\n",
      "Query: 'risk management'\n",
      "  Result 1:\n",
      "    Source: aml-ctf-statement-attention-of-cbl-transfer-agent-data.pdf\n",
      "    Page: 3\n",
      "    Content preview: 4 \n",
      "Customer Risk Assessment \n",
      "For the purposes of undertaking Customer Due Diligence, CBL has risk-based policies \n",
      "and procedures in relation to AML, C...\n",
      "    --------------------------------------------------\n",
      "  Result 2:\n",
      "    Source: cbl-aml-questionnaire-data.pdf\n",
      "    Page: 5\n",
      "    Content preview: Wolfsberg Group Correspondent Banking Due Diligence Questionnaire (CBDDQ) V1.4\n",
      "Â© The Wolfsberg Group 2023 Page 6 CBDDQ V1.4\n",
      "49 i Define the process fo...\n",
      "    --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Test the retrieval system\n",
    "print(\"Testing document retrieval...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with a financial/regulatory query relevant to Deutsche BÃ¶rse\n",
    "test_queries = [\n",
    "    \"investment fund regulations\",\n",
    "    \"trading restrictions\",\n",
    "    \"compliance requirements\",\n",
    "    \"risk management\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    # Perform similarity search\n",
    "    relevant_docs = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        print(f\"  Result {i}:\")\n",
    "        print(f\"    Source: {doc.metadata.get('file_name', 'Unknown')}\")\n",
    "        print(f\"    Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "        print(f\"    Content preview: {doc.page_content[:150].strip()}...\")\n",
    "        print(\"    \" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f39caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vector store and documents...\n",
      "==================================================\n",
      "âœ“ Vector store saved to 'vector_store' folder\n",
      "âœ“ Processed documents saved to 'processed_documents.pkl'\n",
      "âœ“ Embedding configuration saved to 'embedding_config.pkl'\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ INGESTION PIPELINE COMPLETE!\n",
      "==================================================\n",
      "âœ… Processed 30 documents\n",
      "âœ… Created 87 searchable chunks\n",
      "âœ… Generated 384-dimensional embeddings\n",
      "âœ… Vector store ready for RAG implementation\n",
      "\n",
      "Next step: Use the saved vector store in your RAG pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Save vector store for later use\n",
    "import pickle\n",
    "\n",
    "print(\"Saving vector store and documents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save vector store to disk (FAISS format)\n",
    "vectorstore.save_local(\"vector_store\")\n",
    "print(\"âœ“ Vector store saved to 'vector_store' folder\")\n",
    "\n",
    "# Save processed documents for reference\n",
    "with open(\"processed_documents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_documents, f)\n",
    "print(\"âœ“ Processed documents saved to 'processed_documents.pkl'\")\n",
    "\n",
    "# Save embedding model info for consistency\n",
    "embedding_info = {\n",
    "    'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'embedding_dimension': 384,\n",
    "    'chunk_size': 1000,\n",
    "    'chunk_overlap': 200,\n",
    "    'total_chunks': len(chunked_documents)\n",
    "}\n",
    "\n",
    "with open(\"embedding_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embedding_info, f)\n",
    "print(\"âœ“ Embedding configuration saved to 'embedding_config.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸŽ‰ INGESTION PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Processed {len(all_documents)} documents\")\n",
    "print(f\"âœ… Created {len(chunked_documents)} searchable chunks\") \n",
    "print(f\"âœ… Generated 384-dimensional embeddings\")\n",
    "print(\"âœ… Vector store ready for RAG implementation\")\n",
    "print(\"\\nNext step: Use the saved vector store in your RAG pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
